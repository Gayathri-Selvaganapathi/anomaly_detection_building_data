{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import optuna\n",
    "from utils import TrainingConfig, Loader\n",
    "from typing import Union, List, Tuple\n",
    "import time\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "# device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"zone_022_co2\",\"zone_052_co2\",\"zone_072_co2\",\"mel_S\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optuna tuning\n",
    "\n",
    "https://github.com/elena-ecn/optuna-optimization-for-PyTorch-CNN/blob/main/optuna_optimization.py   \n",
    "\n",
    "https://medium.com/pytorch/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuning:\n",
    "    def __init__(self, config:TrainingConfig) -> None:\n",
    "        self.L = Loader(config)\n",
    "\n",
    "    def train(self, model, optimizer):\n",
    "        model.train()  # Set the module in training mode (only affects certain modules)\n",
    "\n",
    "        for batch_i, (data, target) in enumerate(self.L.train_dataloader):      # For each batch\n",
    "            optimizer.zero_grad()                                               # Clear gradients\n",
    "            output = model(data.type(torch.float32))                            # Forward propagation\n",
    "            loss = F.mse_loss(output, target.type(torch.float32))               # Compute loss\n",
    "            loss.backward()                                                     # Compute gradients\n",
    "            optimizer.step()                                                    # Update weights\n",
    "\n",
    "    def test(self, model):\n",
    "        model.eval()         # Set the module in evaluation mode (only affects certain modules)\n",
    "        errors = []\n",
    "        with torch.no_grad():  \n",
    "            for batch_i, (data, target) in enumerate(self.L.test_dataloader):  \n",
    "                output = model(data.type(torch.float32))                \n",
    "                error = F.mse_loss(output, target.type(torch.float32)).mean().item()  \n",
    "                errors.append(error)\n",
    "\n",
    "        accuracy_test = sum(errors) / len(errors)\n",
    "\n",
    "        return accuracy_test\n",
    "    \n",
    "    def xgb_train(self, model:XGBRegressor):\n",
    "        model.fit(np.array(self.L.X_train).reshape(len(self.L.X_train),-1), np.array(self.L.y_train))\n",
    "\n",
    "    def xgb_test(self, model:XGBRegressor):\n",
    "        y_pred = model.predict(np.array(self.L.X_test).reshape(len(self.L.X_test),-1))\n",
    "        score = mean_squared_error(y_true=self.L.y_test, y_pred=y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.config = config\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(1, self.hidden_dim, num_layers=config.n_layers, batch_first=True) # (B, T, C)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSMT Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define range of values to be tested for the hyperparameters\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 96, 128, 196, 256])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [48, 64, 96, 128, 196, 256])\n",
    "    block_size = trial.suggest_categorical(\"block_size\", [16, 32, 48, 64, 96, 128])\n",
    "\n",
    "    TC = TrainingConfig(\n",
    "        dataset_name=\"co2-1\",\n",
    "        column_name='zone_052_co2',\n",
    "        model_type=\"LSMT\",\n",
    "        standardized=True,\n",
    "        export=True,\n",
    "        epochs=3,\n",
    "        batch_size=batch_size,\n",
    "        block_size=block_size,\n",
    "        hidden_dim=hidden_size,\n",
    "        n_layers = n_layers,\n",
    "        lr=lr,\n",
    "        colsample_bytree=None,\n",
    "        subsample=None,\n",
    "        n_estimators=None,\n",
    "        max_depth=None\n",
    "    )\n",
    "\n",
    "    # Generate the model\n",
    "    model = LSTM(config=TC)\n",
    "\n",
    "    # Generate the optimizers\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95))\n",
    "\n",
    "    HT = HyperparameterTuning(config=TC)\n",
    "\n",
    "    # Training of the model\n",
    "    for epoch in range(TC.epochs):\n",
    "        HT.train(model, optimizer)  # Train the model\n",
    "        accuracy = HT.test(model)   # Evaluate the model\n",
    "\n",
    "        # For pruning (stops trial early if not promising)\n",
    "        trial.report(accuracy, epoch)\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_jobs=10, n_trials=80)\n",
    "\n",
    "# print(study.best_trials[0].values[0])\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best trial:\n",
    "# {'batch_size': 64, 'lr': 0.00044187545383663645, 'n_layers': 2, 'hidden_size': 256, 'block_size': 128}\n",
    "#   Value:  0.024644049937310425\n",
    "# {'batch_size': 64, 'lr': 0.0006091567246063254, 'n_layers': 2, 'hidden_size': 256, 'block_size': 128}\n",
    "#   Value:  0.024584050078826827\n",
    "# {'batch_size': 32,\n",
    "#  'lr': 0.0017066009255505266,\n",
    "#  'n_layers': 1,\n",
    "#  'hidden_size': 256,\n",
    "#  'block_size': 64} values: 0.024468314413269215"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial):\n",
    "    # Define range of values to be tested for the hyperparameters\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 6)\n",
    "    n_estimators = trial.suggest_int(\"n_estimator\", 100, 500)\n",
    "    block_size = trial.suggest_categorical(\"block_size\", [32, 48 ,64, 96])\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.7, 1.0)\n",
    "\n",
    "    TC = TrainingConfig(\n",
    "        # dataset_name=\"ele-2\",\n",
    "        dataset_name=\"co2-2\",\n",
    "        column_name='zone_072_co2',\n",
    "        # column_name='mels_S',\n",
    "        model_type=\"XGB\",\n",
    "        standardized = True,\n",
    "        epochs=3,\n",
    "        batch_size = 20, # Not used\n",
    "        block_size = block_size,\n",
    "        hidden_dim = None,\n",
    "        n_layers = None,\n",
    "        max_depth=max_depth,\n",
    "        lr=None,\n",
    "        n_estimators = n_estimators,\n",
    "        subsample = subsample\n",
    "    )\n",
    "\n",
    "    # Generate the model\n",
    "    model = XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, subsample=subsample)\n",
    "\n",
    "    HT = HyperparameterTuning(config=TC)\n",
    "\n",
    "    # Training of the model\n",
    "    for epoch in range(TC.epochs):\n",
    "        HT.xgb_train(model)                # Train the model\n",
    "        accuracy = HT.xgb_test(model)      # Evaluate the model\n",
    "\n",
    "        # For pruning (stops trial early if not promising)\n",
    "        trial.report(accuracy, epoch)\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(xgb_objective, n_jobs=5, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(study.best_trials[0].values[0])\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.03524799340451928\n",
    "# {'max_depth': 4,\n",
    "#  'n_estimator': 50,\n",
    "#  'block_size': 128,\n",
    "#  'subsample': 0.9213629465796025}\n",
    "\n",
    "# 0.13585137826503266\n",
    "# {'max_depth': 3,\n",
    "#  'n_estimator': 282,\n",
    "#  'block_size': 32,\n",
    "#  'subsample': 0.9423739511390844}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_config = TrainingConfig(\n",
    "    dataset_name=\"co2-2\",\n",
    "    column_name='zone_022_co2',\n",
    "    model_type=\"XGB\",\n",
    "    standardized = True,\n",
    "    epochs=None,\n",
    "    batch_size = None,\n",
    "    block_size = 128,\n",
    "    max_depth=4,\n",
    "    n_estimators=50,\n",
    "    lr=None,\n",
    "    subsample=0.9213629465796025\n",
    ")\n",
    "\n",
    "lsmt_config = TrainingConfig(\n",
    "    dataset_name=\"co2-2\",\n",
    "    column_name='zone_022_co2',\n",
    "    model_type=\"LSMT\",\n",
    "    standardized = True,\n",
    "    epochs = 5,\n",
    "    batch_size = 32, \n",
    "    block_size = 64,\n",
    "    hidden_dim = 256,\n",
    "    n_layers = 1,\n",
    "    lr=0.0017066009255505266,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer: \n",
    "    def __init__(self):\n",
    "        self.config: TrainingConfig\n",
    "\n",
    "    def get_model(self, model_type:str):\n",
    "        if model_type == \"XGB\":\n",
    "            return XGBRegressor(\n",
    "                n_estimators=self.config.n_estimators,\n",
    "                max_depth=self.config.max_depth,\n",
    "                subsample=self.config.subsample,\n",
    "            )\n",
    "        elif model_type == \"LSMT\":\n",
    "            return LSTM(config=self.config)\n",
    "        else:\n",
    "            raise Exception(f'No Model Type Selected: {model_type}')\n",
    "        \n",
    "    def get_training_data(self, model_type) -> Union[Tuple[np.ndarray,np.ndarray], DataLoader]:\n",
    "        if model_type == \"XGB\":\n",
    "            X_train = np.array(self.L.X_train).reshape(len(self.L.X_train),-1)\n",
    "            y_train = np.array(self.L.y_train)\n",
    "            return X_train,y_train\n",
    "        elif model_type == \"LSMT\":\n",
    "            return self.L.train_dataloader\n",
    "        else:\n",
    "            raise Exception(f'No Model Type Selected: {model_type}')\n",
    "\n",
    "    def lsmt_training(self):\n",
    "        model = self.get_model(self.config.model_type)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=self.config.lr, betas=(0.9, 0.95))\n",
    "        model.train()  # Set the module in training mode (only affects certain modules)\n",
    "        training_data_loader = self.get_training_data(self.config.model_type)\n",
    "\n",
    "        for epoch in range(self.config.epochs): # type: ignore\n",
    "            # print(f'lsmt_training epoch: {epoch+1}')\n",
    "            for batch_i, (data, target) in enumerate(training_data_loader):         # For each batch\n",
    "                optimizer.zero_grad()                                               # Clear gradients\n",
    "                output = model(data.type(torch.float32))                            # Forward propagation\n",
    "                loss = F.mse_loss(output, target.type(torch.float32))               # Compute loss\n",
    "                loss.backward()                                                     # Compute gradients\n",
    "                optimizer.step()                                                    # Update weights\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def xgb_fit_training(self):\n",
    "        model = self.get_model(self.config.model_type)\n",
    "        X_train, y_train = self.get_training_data(self.config.model_type)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def run(self, input_config:TrainingConfig):\n",
    "        self.config, model_type = input_config, input_config.model_type\n",
    "        self.L = Loader(input_config)\n",
    "        print(f'Starting Training of {model_type} on dataset: {self.config.dataset_name} column: {self.config.column_name}')\n",
    "        MODEL = None\n",
    "        st = time.perf_counter()\n",
    "        if model_type == \"XGB\":\n",
    "            MODEL = self.xgb_fit_training()\n",
    "        elif model_type == \"LSMT\":\n",
    "            MODEL = self.lsmt_training()\n",
    "        else:\n",
    "            raise Exception(f'Failed to train: {model_type}')\n",
    "        \n",
    "        print(f'Completed Training of {model_type} in {(time.perf_counter()-st)/60:.2f} min')\n",
    "        return MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsmt_MODEL = Trainer().run(lsmt_config)\n",
    "xgb_MODEL = Trainer().run(xgb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validate:\n",
    "    def __init__(self) -> None:\n",
    "        self.config: TrainingConfig\n",
    "\n",
    "    def get_testing_data(self, model_type) -> Union[Tuple[np.ndarray,np.ndarray], DataLoader]:\n",
    "        if model_type == \"XGB\":\n",
    "            X_test = np.array(self.L.X_test).reshape(len(self.L.X_test),-1)\n",
    "            y_test = np.array(self.L.y_test)\n",
    "            return X_test, y_test\n",
    "        elif model_type == \"LSMT\":\n",
    "            return self.L.test_dataloader\n",
    "        else:\n",
    "            raise Exception(f'No Model Type Selected: {model_type}')\n",
    "        \n",
    "    @ torch.no_grad()\n",
    "    def validate_lsmt(self) -> dict:\n",
    "        self.MODEL.eval()  # Set the module in training mode (only affects certain modules)\n",
    "        testing_data_loader = self.get_testing_data(self.config.model_type)\n",
    "\n",
    "        score_arr = np.zeros((2,len(testing_data_loader)))\n",
    "\n",
    "        for batch_i, (data, target) in enumerate(testing_data_loader):  \n",
    "            out = self.MODEL(data.type(torch.float32))  \n",
    "            mse = mean_squared_error(y_true=target.squeeze().numpy(), y_pred=out.squeeze().numpy())\n",
    "            mape = mean_absolute_percentage_error(y_true=target.squeeze().numpy(), y_pred=out.squeeze().numpy())\n",
    "            score_arr[0,batch_i] = mse\n",
    "            score_arr[1,batch_i] = mape  \n",
    "\n",
    "        return {\"mse\":score_arr[0].mean(), \"mape\":score_arr[1].mean()}\n",
    "\n",
    "    def validate_xgb(self) -> dict:\n",
    "        X_test, y_test = self.get_testing_data(self.config.model_type)\n",
    "        y_pred = self.MODEL.predict(X_test)\n",
    "        mse = mean_squared_error(y_true=np.array([y_test.flatten()]), y_pred=np.array([y_pred]))\n",
    "        mape = mean_absolute_percentage_error(y_true=np.array([y_test.flatten()]), y_pred=np.array([y_pred]))\n",
    "        return {\"mse\":mse, \"mape\":mape}\n",
    "    \n",
    "    def run(self, trained_model, input_config:TrainingConfig ) -> dict:\n",
    "        # Set up\n",
    "        self.MODEL, self.config = trained_model, input_config\n",
    "        self.L = Loader(input_config)\n",
    "        print(f'Starting validation of {self.config.model_type}')\n",
    "        \n",
    "        # Run Validation with test data\n",
    "        if self.config.model_type == \"XGB\":\n",
    "            res = self.validate_xgb()\n",
    "        elif self.config.model_type == \"LSMT\":\n",
    "            res = self.validate_lsmt()\n",
    "        else:\n",
    "            raise Exception(f'Failed to train: {self.config.model_type}')\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate().run(lsmt_MODEL, lsmt_config)\n",
    "Validate().run(xgb_MODEL, xgb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compare:\n",
    "    def __init__(self) -> None:\n",
    "        self.datasets:list = self.get_datasets()\n",
    "        self.res = []\n",
    "        self.xgb_config = TrainingConfig(\n",
    "            dataset_name=\"co2-1\",\n",
    "            column_name='zone_072_co2',\n",
    "            model_type=\"XGB\",\n",
    "            standardized = True,\n",
    "            epochs=None,\n",
    "            batch_size = None,\n",
    "            block_size = 96,\n",
    "            max_depth=5,\n",
    "            lr=0.07044179896591762,\n",
    "            n_estimators=300,\n",
    "            subsample=0.5681930635231892,\n",
    "            colsample_bytree=0.8556373196284424\n",
    "        )\n",
    "        \n",
    "        self.lsmt_config = TrainingConfig(\n",
    "            dataset_name=\"co2-1\",\n",
    "            column_name='zone_072_co2',\n",
    "            model_type=\"LSMT\",\n",
    "            standardized = True,\n",
    "            epochs = 5,\n",
    "            batch_size = 32, \n",
    "            block_size = 64,\n",
    "            hidden_dim = 256,\n",
    "            n_layers = 1,\n",
    "            lr=0.0017066009255505266\n",
    "        )\n",
    "\n",
    "    def get_datasets(self) -> list[dict]:\n",
    "        datasets = []\n",
    "        for dataset in [\"co2-1\",\"co2-2\"]:\n",
    "            for col in [\"zone_022_co2\",\"zone_052_co2\",\"zone_072_co2\"]:\n",
    "                datasets.append({\"dataset_name\":dataset, \"column_name\":col})\n",
    "\n",
    "        datasets.append({\"dataset_name\":\"ele-1\", \"column_name\":\"mels_S\"})\n",
    "        datasets.append({\"dataset_name\":\"ele-2\", \"column_name\":\"mels_S\"})\n",
    "\n",
    "        return datasets\n",
    "    \n",
    "    def update_dataset(self, model_type:str, data:dict) -> TrainingConfig:\n",
    "        cfg = self.xgb_config if model_type==\"XGB\" else self.lsmt_config\n",
    "        cfg.dataset_name = data[\"dataset_name\"]\n",
    "        cfg.column_name = data[\"column_name\"]\n",
    "        return cfg\n",
    "    \n",
    "    def run_trail(self, cfg:TrainingConfig):\n",
    "        st = time.perf_counter()\n",
    "        MODEL = Trainer().run(cfg)\n",
    "        score = Validate().run(MODEL, cfg)\n",
    "\n",
    "        # Update results\n",
    "        results = {\n",
    "            \"model_type\":cfg.model_type, \"dataset\": cfg.dataset_name + \"_\" +cfg.column_name, \"time\":time.perf_counter()-st\n",
    "        }\n",
    "\n",
    "        results.update(score)\n",
    "        \n",
    "        self.res.append(results)\n",
    "    \n",
    "    def run(self, export=False)->pd.DataFrame:\n",
    "\n",
    "        for model in [\"XGB\",\"LSMT\"]:\n",
    "            for data in self.datasets:\n",
    "                trail_cfg = self.update_dataset(model, data)\n",
    "                self.run_trail(trail_cfg)\n",
    "        \n",
    "        df = pd.DataFrame(self.res)\n",
    "\n",
    "        if export:\n",
    "            df.to_csv(\"experiment_results.csv\", index=False)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compare().run(export=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All LSMT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSMTModels:\n",
    "    def __init__(self):\n",
    "        self.datasets:list = self.get_datasets()\n",
    "        self.config = TrainingConfig(\n",
    "            dataset_name=\"co2-1\",\n",
    "            column_name='zone_072_co2',\n",
    "            model_type=\"LSMT\",\n",
    "            standardized = True,\n",
    "            epochs = 5,\n",
    "            batch_size = 32, \n",
    "            block_size = 64,\n",
    "            hidden_dim = 256,\n",
    "            n_layers = 1,\n",
    "            lr=0.0017066009255505266\n",
    "        )\n",
    "\n",
    "    def get_datasets(self) -> list[dict]:\n",
    "        datasets = []\n",
    "        for dataset in [\"co2-1\",\"co2-2\"]:\n",
    "            for col in [\"zone_022_co2\",\"zone_052_co2\",\"zone_072_co2\"]:\n",
    "                datasets.append({\"dataset_name\":dataset, \"column_name\":col})\n",
    "\n",
    "        datasets.append({\"dataset_name\":\"ele-1\", \"column_name\":\"mels_S\"})\n",
    "        datasets.append({\"dataset_name\":\"ele-2\", \"column_name\":\"mels_S\"})\n",
    "\n",
    "        return datasets\n",
    "    \n",
    "    def update_dataset(self, data:dict) -> TrainingConfig:\n",
    "        cfg = self.config\n",
    "        cfg.dataset_name = data[\"dataset_name\"]\n",
    "        cfg.column_name = data[\"column_name\"]\n",
    "        return cfg\n",
    "\n",
    "    def _load_data(self):\n",
    "        self.L = Loader(self.config)\n",
    "        return self.L.get_total_trainer()\n",
    "\n",
    "    def lsmt_training(self):\n",
    "        model = LSTM(config=self.config)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=self.config.lr, betas=(0.9, 0.95))\n",
    "        model.train()  # Set the module in training mode (only affects certain modules)\n",
    "        data_loader = self._load_data()\n",
    "\n",
    "        for epoch in range(self.config.epochs): # type: ignore\n",
    "        # for epoch in range(2): # type: ignore\n",
    "            for batch_i, (data, target) in enumerate(data_loader): \n",
    "                optimizer.zero_grad()                                             \n",
    "                output = model(data.unsqueeze(-1).type(torch.float32))                                                                   \n",
    "                loss = F.mse_loss(output.squeeze(), target.type(torch.float32))               \n",
    "                loss.backward()                                                   \n",
    "                optimizer.step()                                             \n",
    "\n",
    "        return model\n",
    "    \n",
    "    def train_lsmt_models(self, export=False):\n",
    "        for dataset in self.datasets:\n",
    "            st = time.perf_counter()\n",
    "            self.config = self.update_dataset(dataset)\n",
    "            model_file_name = f'{self.config.model_type}_{self.config.dataset_name}_{self.config.column_name}.pk'\n",
    "            MODEL = self.lsmt_training()\n",
    "            if export:\n",
    "                torch.save(MODEL, \"models/\"+model_file_name)\n",
    "                print(f'Saved {model_file_name} in {(time.perf_counter()-st)/60:.1f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets = LSMTModels().train_lsmt_models(export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
